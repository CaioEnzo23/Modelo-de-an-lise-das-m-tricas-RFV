# -*- coding: utf-8 -*-
"""Desafio 7.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/165jIO-d9qII9EO73Zx3Zq7EBorf2Vcwm

#**Setup**

Instalação de Bibliotecas
"""

# Commented out IPython magic to ensure Python compatibility.
# %pip install plotly
# %pip install cufflinks
# %pip install chart-studio

!pip install pandas-profiling
!pip install sidetable
!pip install scikit-learn

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# %pip install KModes

"""Importação das principais bilbiotecas ultilizadas"""

import pandas as pd
import seaborn as sns
import numpy as np
import matplotlib.pyplot as plt

import chart_studio.plotly as py
import cufflinks as cf

import plotly.graph_objects as go
import plotly.express as px

import missingno as msno
from ipywidgets import interact, widgets

from sklearn import datasets
from sklearn.preprocessing import scale, minmax_scale, power_transform
from sklearn.datasets import load_wine

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer

from sklearn.metrics import confusion_matrix
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
import matplotlib.pyplot as plt
from sklearn.metrics import classification_report
from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import GridSearchCV
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import LabelEncoder
from sklearn.impute import SimpleImputer
from sklearn.metrics import accuracy_score, classification_report

from yellowbrick.cluster import KElbowVisualizer, SilhouetteVisualizer, kelbow_visualizer, silhouette_visualizer
from sklearn.mixture import GaussianMixture
from sklearn.cluster import KMeans, DBSCAN, MeanShift
from sklearn.cluster import k_means, dbscan, mean_shift, estimate_bandwidth
from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score
from sklearn.decomposition import PCA
from sklearn.preprocessing import scale

from kmodes.kmodes import KModes
from kmodes.kprototypes import KPrototypes

import string
from ipywidgets import interact

"""#**Análise exploratória dos dados**

Carregue a base de dados
"""

df = pd.read_excel("/content/data.xlsx")

"""Realize uma descrição estatística dos dados"""

descricao = df.describe()
print(descricao)

"""Visualize as distribuições e identifique a relevância das colunas para a análise"""

print("Primeiras linhas do DataFrame:")
print(df.head())

df.hist(bins=30, figsize=(15, 10))
plt.suptitle('Distribuição das Colunas Numéricas')
plt.show()

plt.figure(figsize=(15, 10))
sns.boxplot(data=df)
plt.title('Boxplot das Colunas Numéricas')
plt.show()

for col in df.columns:
    try:
        df[col] = pd.to_numeric(df[col], errors='coerce')
    except:
        pass

plt.figure(figsize=(15, 10))
correlation_matrix = df.corr()
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm')
plt.title('Matriz de Correlação')
plt.show()

"""Verifique a presença de dados nulos, duplicados, outliers e demais inconsistências
nos dados
"""

print("\nDados Nulos por Coluna:")
print(df.isnull().sum())

print("\nNúmero de Linhas Duplicadas:")
print(df.duplicated().sum())

plt.figure(figsize=(15, 10))
sns.boxplot(data=df)
plt.title('Boxplot das Colunas Numéricas')
plt.show()

print("\nEstatísticas Descritivas:")
print(df.describe())

"""Tratando os tipos de dados"""

tipos_de_dados = df.dtypes

print(tipos_de_dados)

df['InvoiceNo'] = df['InvoiceNo'].replace([np.inf, -np.inf], np.nan).fillna(0).astype(int)

print(df['CustomerID'].isnull().sum())  # Check for NaN values
print(df['CustomerID'].isin([np.inf, -np.inf]).sum())

df = df.dropna(subset=['CustomerID'])

df['CustomerID'] = df['CustomerID'].replace([np.inf, -np.inf, np.nan], -1).astype(int)

df = df.astype({"StockCode":"object", "Description":"object",
                 "Quantity":"int","UnitPrice":"float",
                 "CustomerID":"int","Country":"object"})
df.dtypes

df = pd.DataFrame(df)

df['InvoiceDate'] = pd.to_datetime(df['InvoiceDate'])

print(df.dtypes)

"""#**Pré-processamento dos dados**

Realize a normalização dos dados
"""

df['InvoiceDate'].fillna(pd.Timestamp.min, inplace=True)

df['InvoiceDate'] = df['InvoiceDate'].astype(int) / 10**9

scaler = MinMaxScaler()

colunas_normalizadas = ['InvoiceNo', 'StockCode', 'Description', 'Quantity', 'InvoiceDate', 'UnitPrice', 'CustomerID', 'Country']
df[colunas_normalizadas] = scaler.fit_transform(df[colunas_normalizadas])

print(df)

"""Faça uma seleção das variáveis mais relevantes para o modelo"""



"""Remova os dados nulos, duplicados, outliers e inconsistentes"""

print(df.isnull().sum())

df.fillna(0, inplace=True)

def remove_outliers(df, column):
    Q1 = df[column].quantile(0.25)
    Q3 = df[column].quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR
    df = df[(df[column] >= lower_bound) & (df[column] <= upper_bound)]
    return df

for column in df.select_dtypes(include=['float64', 'int64']).columns:
    df = remove_outliers(df, column)

df.to_csv('dados_limpos.csv', index=False)

"""#**Selecione um algoritmo de clusterização**

Escolha um algoritmo adequado para base de dados, como o K0Means, DBSCAN,
Hierarquia ou Mean Shift

**K0Means vai ser o algoritmo ultilizado**

K-Means é o método mais eficiente e prático para os cenários de e-commerce por várias razões:

Simplicidade e Eficiência: É fácil de implementar e computacionalmente eficiente, o que é ideal para grandes volumes de dados comuns em e-commerce.
Facilidade de Interpretação: Produz clusters bem definidos que são facilmente interpretáveis, o que ajuda na tomada de decisões e na criação de estratégias de marketing personalizadas.
Ferramentas de Suporte: Muitas ferramentas de análise de dados e plataformas de machine learning oferecem suporte robusto para K-Means, incluindo visualizações que ajudam a simplificar a análise.
Para uma abordagem inicial, K-Means é a escolha mais recomendada. Caso seja necessário lidar com muitos outliers ou identificar clusters de formas mais complexas, DBSCAN pode ser explorado como uma alternativa.

Encontre a quantidade ideal de clusters através dos métodos de Elbow ou
Silhouette Score
"""

scaler = StandardScaler()
df_scaled = scaler.fit_transform(df.select_dtypes(include=[np.number]))

inertia = []
K = range(1, 11)
for k in K:
    kmeans = KMeans(n_clusters=k, random_state=42)
    kmeans.fit(df_scaled)
    inertia.append(kmeans.inertia_)

plt.figure(figsize=(10, 6))
plt.plot(K, inertia, 'bo-')
plt.xlabel('Número de Clusters')
plt.ylabel('Inertia')
plt.title('Método Elbow')
plt.show()

""" Implemente o algoritmo escolhido"""

x_dim,y_dim = datasets.make_blobs(n_samples=int(5E3), n_features=8, centers=4, cluster_std=8, random_state=0)
df_dim = pd.DataFrame(x_dim, columns=list(string.ascii_uppercase[:x_dim.shape[1]]))
df_dim.head()

pca = PCA()
pca_components = pca.fit_transform(df_dim)

df_pca = pd.DataFrame(pca_components, columns=[f'PC{i+1}' for i in range(pca_components.shape[1])])
df_pca.head()

pd.DataFrame(
    {'explained_var': pca.explained_variance_ratio_, 'explained_var_cumsum': pca.explained_variance_ratio_.cumsum()},
    index=df_pca.columns
)

sns.scatterplot(data=df_pca, x='PC1', y='PC2', hue=y_dim, **scatter_kwargs);
plt.title('PCA')
plt.show()

labels = k_means(df_pca.iloc[:,:3], 4)[1]
sns.scatterplot(data=df_pca, x='PC1', y='PC2', hue=y_dim, **scatter_kwargs);

px.scatter_3d(df_pca, x='PC1', y='PC2', z='PC3', color=labels, template='plotly_dark')

"""#**Analise os clusters obtidos**

Identifique os padrões e características em comum entre os clientes
"""

model = KMeans(n_clusters=10)
model.fit(x)

labels = model.labels_

silhouette_score(x, labels)

davies_bouldin_score(x, labels)

calinski_harabasz_score(x, labels)

""" Plote gráficos para auxiliar na análise"""

kelbow_visualizer?

kelbow_visualizer(KMeans(), x, k=10)

silhouette_visualizer?

silhouette_visualizer(KMeans(n_clusters=4), x)

cluster_metrics = silhouette_score, davies_bouldin_score, calinski_harabasz_score
cluster_metrics_results = []

for k in range(2, 11):
    model = KMeans(n_clusters=k, random_state=0)
    labels = model.fit_predict(x)
    cluster_results_dict = {'k':k}
    cluster_results_dict['inertia'] = model.inertia_
    for metric in cluster_metrics:
        cluster_results_dict[metric.__name__] = metric(x, labels)
    cluster_metrics_results.append(cluster_results_dict)

    cluster_metrics_results

pd.DataFrame(cluster_metrics_results).set_index('k').style.background_gradient()

"""#**Dados de Clientes**

Clientes que compram os mesmos produtos
"""

df = pd.read_excel("/content/data.xlsx")

purchase_matrix = df.pivot_table(index='CustomerID', columns='Description', aggfunc='size', fill_value=0) # Changed 'CustumerID' to 'CustomerID'

scaler = StandardScaler()
purchase_matrix_scaled = scaler.fit_transform(purchase_matrix)

pca = PCA(n_components=2)
purchase_matrix_pca = pca.fit_transform(purchase_matrix_scaled)

inertia = []
K_range = range(1, 11)
for k in K_range:
    kmeans = KMeans(n_clusters=k, random_state=42)
    kmeans.fit(purchase_matrix_pca)
    inertia.append(kmeans.inertia_)

plt.figure(figsize=(8, 4))
plt.plot(K_range, inertia, marker='o')
plt.title('Método do Cotovelo')
plt.xlabel('Número de Clusters')
plt.ylabel('Inertia')
plt.show()

kmeans = KMeans(n_clusters=4, random_state=42)
clusters = kmeans.fit_predict(purchase_matrix_pca)

purchase_matrix['Cluster'] = clusters

for cluster in range(4):
    print(f"Cluster {cluster}")
    cluster_data = purchase_matrix[purchase_matrix['Cluster'] == cluster]
    common_products = cluster_data.drop('Cluster', axis=1).sum().sort_values(ascending=False).head(10)
    print(common_products)

plt.figure(figsize=(10, 6))
sns.scatterplot(x=purchase_matrix_pca[:, 0], y=purchase_matrix_pca[:, 1], hue=clusters, palette='viridis')
plt.title('Clusters de Clientes com Base nos Produtos Comprados')
plt.xlabel('PCA Component 1')
plt.ylabel('PCA Component 2')
plt.show()

"""Clientes que possuem a mesma frequência de compras"""

df = pd.read_excel("/content/data.xlsx")

df['InvoiceDate'] = pd.to_datetime(df['InvoiceDate'])
df['year_month'] = df['InvoiceDate'].dt.to_period('M')
frequency = df.groupby(['CustomerID', 'year_month']).size().reset_index(name='UnitPrice')
frequency_summary = frequency.groupby('CustomerID')['UnitPrice'].sum().reset_index()

scaler = StandardScaler()
frequency_summary_scaled = scaler.fit_transform(frequency_summary[['UnitPrice']])

inertia = []
K_range = range(1, 11)
for k in K_range:
    kmeans = KMeans(n_clusters=k, random_state=42)
    kmeans.fit(frequency_summary_scaled)
    inertia.append(kmeans.inertia_)

plt.figure(figsize=(8, 4))
plt.plot(K_range, inertia, marker='o')
plt.title('Método do Cotovelo')
plt.xlabel('Número de Clusters')
plt.ylabel('Inertia')
plt.show()

kmeans = KMeans(n_clusters=4, random_state=42)
clusters = kmeans.fit_predict(frequency_summary_scaled)

frequency_summary['Cluster'] = clusters

for cluster in range(4):
    print(f"Cluster {cluster}")
    cluster_data = frequency_summary[frequency_summary['Cluster'] == cluster]
    print(cluster_data.describe())

plt.figure(figsize=(10, 6))
sns.scatterplot(x=frequency_summary_scaled[:, 0], y=frequency_summary_scaled[:, 0], hue=clusters, palette='viridis')
plt.title('Clusters de Clientes com Base na Frequência de Compras')
plt.xlabel('Frequência Normalizada')
plt.ylabel('Frequência Normalizada')
plt.show()

"""Clientes que gastam mais dinheiro em suas compras"""

df = pd.read_excel("/content/data.xlsx")

monetary = df.groupby('CustomerID')['UnitPrice'].sum().reset_index()

scaler = StandardScaler()
monetary_scaled = scaler.fit_transform(monetary[['UnitPrice']])

inertia = []
K_range = range(1, 11)
for k in K_range:
    kmeans = KMeans(n_clusters=k, random_state=42)
    kmeans.fit(monetary_scaled)
    inertia.append(kmeans.inertia_)

plt.figure(figsize=(8, 4))
plt.plot(K_range, inertia, marker='o')
plt.title('Método do Cotovelo')
plt.xlabel('Número de Clusters')
plt.ylabel('Inertia')
plt.show()

kmeans = KMeans(n_clusters=4, random_state=42)
clusters = kmeans.fit_predict(monetary_scaled)

monetary['Cluster'] = clusters

for cluster in range(4):
    print(f"Cluster {cluster}")
    cluster_data = monetary[monetary['Cluster'] == cluster]
    print(cluster_data.describe())

plt.figure(figsize=(10, 6))
sns.scatterplot(x=monetary_scaled[:, 0], y=monetary_scaled[:, 0], hue=clusters, palette='viridis')
plt.title('Clusters de Clientes com Base no Valor Monetário Gasto')
plt.xlabel('Valor Monetário Normalizado')
plt.ylabel('Valor Monetário Normalizado')
plt.show()

"""#**Interpretação dos resultados obtidos**

**Descreva o perfil de compras dos clientes de cada cluster**

O cluster do "Clientes que compram os mesmos produtos" são bem randomicos e dispersos ou seja são bem imprevisiveis porém os clusters "Clientes que possuem a mesma frequência de compras" e "Clientes que gastam mais dinheiro em suas compras" são bem lineares ou seja tem maior taxa de sucesso trabalhar com eles para melhorar a preformace do E-comerce

**Justifique como essa análise pode ser útil para empresa para segmentação de
seus clientes e personalização das campanhas de marketing**

A análise de clustering dos clientes com base no valor monetário gasto pode ser extremamente útil para uma empresa de várias maneiras, especialmente para segmentação de clientes e personalização das campanhas de marketing

1. **Segmentação de Clientes**
> a. Identificação de Grupos Distintos
*   Descrição: A análise de clustering permite identificar grupos distintos de clientes com base em seus padrões de gastos.
*   Benefício: Entender esses grupos ajuda a empresa a tratar cada segmento de maneira específica, reconhecendo que os clientes não são homogêneos e possuem comportamentos de compra variados.





> b. Priorização de Recursos
*   Descrição: Com a segmentação, a empresa pode alocar recursos de marketing de maneira mais eficiente, focando nos segmentos mais lucrativos.
*   Benefício: Isso maximiza o retorno sobre o investimento (ROI) em campanhas de marketing, pois os esforços são direcionados para os clientes que têm maior probabilidade de gerar receitas significativas.

2. **Personalização das Campanhas de Marketing**
> a. Campanhas Direcionadas
*   Descrição: Conhecendo os perfis de gastos dos diferentes clusters, a empresa pode criar campanhas de marketing personalizadas que ressoem melhor com cada grupo.
*   Benefício: As mensagens de marketing personalizadas são mais eficazes em capturar a atenção dos clientes e incentivá-los a realizar compras, aumentando a taxa de conversão.

>  b. Ofertas Personalizadas
*   Descrição: Para os "big spenders", a empresa pode oferecer programas de fidelidade exclusivos e benefícios adicionais. Para os "low spenders", promoções e descontos significativos podem ser usados para incentivar maiores compras.
*   Benefício: Ofertas personalizadas aumentam a satisfação e a lealdade do cliente, além de incentivar compras repetidas.

>  c. Estratégias de Retenção
*   Descrição: Clientes que gastam menos ou compram com pouca frequência podem ser alvo de campanhas de reengajamento, como e-mails personalizados e ofertas especiais.
*  Benefício: Isso ajuda a reduzir a taxa de churn, mantendo mais clientes ativos e aumentando a receita ao longo do tempo.

3. **Melhoria da Experiência do Cliente**

> a. Atendimento Personalizado
*  Descrição: Com a segmentação clara, a empresa pode oferecer um atendimento ao cliente mais personalizado, ajustando o serviço para atender melhor as necessidades de cada segmento.
*  Benefício: Um atendimento personalizado aumenta a satisfação do cliente e pode transformar compradores ocasionais em clientes leais.

4. **Previsão e Planejamento**

> a. Planejamento de Estoque
*  Descrição: Entendendo quais produtos são mais comprados pelos diferentes clusters, a empresa pode planejar melhor o estoque, garantindo que os produtos certos estejam disponíveis para os clientes certos.
*  Benefício: Isso reduz o risco de excesso ou falta de estoque, otimiza a logística e melhora a eficiência operacional.

> b. Desenvolvimento de Novos Produtos
*  Descrição: A análise de clusters pode revelar oportunidades de mercado para o desenvolvimento de novos produtos que atendam às necessidades específicas de diferentes segmentos de clientes.
*  Benefício: Isso permite à empresa inovar de maneira direcionada, criando produtos que têm maior probabilidade de sucesso no mercado.

5. **Análise de Competitividade**

> a. Benchmarking Interno
*  Descrição: A empresa pode comparar o desempenho de diferentes clusters ao longo do tempo, ajustando suas estratégias de marketing conforme necessário para melhorar o desempenho.
*  Benefício: Isso proporciona uma vantagem competitiva ao permitir que a empresa reaja rapidamente às mudanças no comportamento do cliente e nas tendências do mercado.

**Conclusão**

A análise de clustering baseada no valor monetário gasto pelos clientes oferece uma riqueza de insights que podem ser usados para segmentar a base de clientes de maneira eficaz e personalizar campanhas de marketing. Isso não só melhora a eficácia das estratégias de marketing, mas também aumenta a satisfação do cliente, promove a lealdade e maximiza a receita da empresa.

**Sugira ações possíveis com base nas ações realizadas**

Concentrar os Esforços do Marketing nos "Clientes que possuem a mesma frequência de compras" e "Clientes que gastam mais dinheiro em suas compras" usando os dados que tem o qual são mais precisos e lineares
"""
